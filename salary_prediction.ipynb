{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load data\n",
    "train_data = pd.read_csv('./usjobs_train.csv')\n",
    "test_data = pd.read_csv('./usjobs_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        115000.000\n",
      "1        185000.000\n",
      "2         84500.000\n",
      "3        111625.000\n",
      "4        102690.400\n",
      "            ...    \n",
      "33243     47206.495\n",
      "33244     79741.000\n",
      "33245    119908.000\n",
      "33246    115000.000\n",
      "33247    155000.000\n",
      "Name: Mean_Salary, Length: 33248, dtype: float64\n",
      "0        105000\n",
      "1        105000\n",
      "2        105000\n",
      "3        105000\n",
      "4        105000\n",
      "          ...  \n",
      "22161    105000\n",
      "22162    105000\n",
      "22163    105000\n",
      "22164    105000\n",
      "22165    105000\n",
      "Name: Mean_Salary, Length: 22166, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# note that for train data, the mean_salary values contribute to an additional column compared to test data, we take the mean_salary column out of train_data as train_target\n",
    "\n",
    "train_target = train_data['Mean_Salary']\n",
    "train_data = train_data.drop(columns=['ID','Mean_Salary'])\n",
    "print(train_target)\n",
    "test_target = pd.read_csv('./usjobs_sample_submission.csv')['Mean_Salary']\n",
    "print(test_target)\n",
    "# now you notice that no actual salary for the test data is given, so that we only need to train and evaluate model performance on train data\n",
    "test_data = test_data.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job                     0\n",
      "Jobs_Group              0\n",
      "Profile             21107\n",
      "Remote              19319\n",
      "Company                 9\n",
      "Location               13\n",
      "City                 3824\n",
      "State                3112\n",
      "Frecuency_Salary        0\n",
      "Skills                  0\n",
      "Sector               7214\n",
      "Sector_Group         7214\n",
      "Revenue             18318\n",
      "Employee            12799\n",
      "Company_Score        8762\n",
      "Reviews              8762\n",
      "Director            20785\n",
      "Director_Score      21924\n",
      "URL                 16033\n",
      "dtype: int64\n",
      "Job                 0.518136\n",
      "Jobs_Group          0.000421\n",
      "Profile             0.000247\n",
      "Remote              0.000144\n",
      "Company             0.421042\n",
      "Location            0.377373\n",
      "City                0.100292\n",
      "State               0.001792\n",
      "Frecuency_Salary    0.000150\n",
      "Skills              0.324982\n",
      "Sector              0.005301\n",
      "Sector_Group        0.001076\n",
      "Revenue             0.000603\n",
      "Employee            0.000440\n",
      "Company_Score       0.001552\n",
      "Reviews             0.056359\n",
      "Director            0.209661\n",
      "Director_Score      0.006711\n",
      "URL                 0.298983\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check if any null value in train_data\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# turns out there are many null values in multiple columns. We should fill the columns of strings with 'None' and columns of numbers with 0, otherwise we have to drop a lot of them\n",
    "# but first, there are columns of information that are probably not useful for prediction, let's check, by counting the number of unique values in each column\n",
    "print(train_data.nunique()/train_data.count())\n",
    "# a ratio higher than 0.25 is potentially not a good indicator\n",
    "\n",
    "# drop the columns with a ratio higher than 0.25\n",
    "NG_columns = train_data.columns[(train_data.nunique()/train_data.count() > 0.25)]#.index\n",
    "train_data = train_data.drop(columns=NG_columns)\n",
    "test_data = test_data.drop(columns=NG_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs_Group           object\n",
      "Profile              object\n",
      "Remote               object\n",
      "City                 object\n",
      "State                object\n",
      "Frecuency_Salary     object\n",
      "Sector               object\n",
      "Sector_Group         object\n",
      "Revenue              object\n",
      "Employee             object\n",
      "Company_Score       float64\n",
      "Reviews             float64\n",
      "Director             object\n",
      "Director_Score      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# now fill in the null values. For string type columns, fill in with 'None', for numerical type columns, fill in with 0\n",
    "# first check whether the columns are numerical or string type\n",
    "print(train_data.dtypes)\n",
    "string_columns = train_data.columns[train_data.dtypes == 'object']\n",
    "numerical_columns = train_data.columns[train_data.dtypes != 'object']\n",
    "train_data[string_columns] = train_data[string_columns].fillna('None')\n",
    "train_data[numerical_columns] = train_data[numerical_columns].fillna(0)\n",
    "\n",
    "\n",
    "test_data[string_columns] = test_data[string_columns].fillna('None')\n",
    "test_data[numerical_columns] = test_data[numerical_columns].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical data to numerical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoded_train_data = train_data.copy()\n",
    "encoded_test_data = test_data.copy()\n",
    "labelencoder = LabelEncoder()\n",
    "mapping = {}\n",
    "print(encoded_train_data.dtypes)\n",
    "for column in string_columns:\n",
    "    encoded_train_data[column] = labelencoder.fit_transform(encoded_train_data[column])\n",
    "    mapping[column] = dict(zip(labelencoder.classes_, labelencoder.transform(labelencoder.classes_)))\n",
    "    # for values in encoded_test_data[column], if the value is not in the mapping, then we set it to be 'None'\n",
    "    encoded_test_data[column] = encoded_test_data[column].apply(lambda x: x if x in mapping[column] else 'None')\n",
    "    encoded_test_data[column] = labelencoder.transform(encoded_test_data[column])\n",
    "\n",
    "print(mapping)\n",
    "print(encoded_train_data.head())\n",
    "print(encoded_train_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33020.98790570812\n"
     ]
    }
   ],
   "source": [
    "# establish accuracy baseline\n",
    "train_label_mean = train_target.mean()\n",
    "mean_absolute_error_baseline = abs(train_target - train_label_mean).mean()\n",
    "print(mean_absolute_error_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try all basic machine learning models to fit encoded_train_data to train_target\n",
    "# from sklearn import linear_model, svm\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# for model in [linear_model.LinearRegression(),\n",
    "#               linear_model.Ridge(),\n",
    "#               linear_model.Lasso(),\n",
    "#               linear_model.BayesianRidge(),\n",
    "#               linear_model.ElasticNet(),\n",
    "#               linear_model.LassoLars(),\n",
    "#               linear_model.ARDRegression(),\n",
    "#               linear_model.PassiveAggressiveRegressor(),\n",
    "#               linear_model.TheilSenRegressor(),\n",
    "#               linear_model.HuberRegressor(),\n",
    "#               linear_model.RANSACRegressor(),\n",
    "#               svm.SVR()]:\n",
    "#     model.fit(encoded_train_data, train_target)\n",
    "#     predictions = model.predict(encoded_train_data)\n",
    "#     print(model)\n",
    "#     print(mean_absolute_error(train_target, predictions))\n",
    "\n",
    "# obviously these basic models are not good enough, let's try some ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTrees\n",
      "2669.084444929978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor, VotingRegressor, StackingRegressor, IsolationForest\n",
    "\n",
    "estimators = [\n",
    "    # ('RandomForest', RandomForestRegressor(n_estimators=100)),\n",
    "    # ('GradientBoosting', GradientBoostingRegressor(n_estimators=100)),\n",
    "    ('ExtraTrees', ExtraTreesRegressor(n_estimators=100)),\n",
    "    # ('AdaBoost', AdaBoostRegressor(n_estimators=100)),\n",
    "    # ('Bagging', BaggingRegressor(n_estimators=100)),\n",
    "    # ('IsolationForest', IsolationForest(n_estimators=100))\n",
    "]\n",
    "\n",
    "for name, model in estimators:\n",
    "    model.fit(encoded_train_data, train_target)\n",
    "    predictions = model.predict(encoded_train_data)\n",
    "    print(name)\n",
    "    print(mean_absolute_error(train_target, predictions))\n",
    "\n",
    "# final conclusion: ExtraTreesRegressor might be the best model, wth mean absolute error in train data of 2669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.read_csv('./usjobs_sample_submission.csv')\n",
    "# preprocess test data and make predictions as y_pred\n",
    "test_result['Mean_Salary'] = model.predict(encoded_test_data)\n",
    "test_result.to_csv('./test_result.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
